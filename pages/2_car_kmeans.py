# -*- coding: utf-8 -*-
"""ì°¨ëª…ë³„ K-means êµ°ì§‘ ë¶„ì„ (k ìë™ì„ ì •, ëª¨ë“  ê²°ê³¼Â·í”„ë¡œíŒŒì¼ ê°€ë¡œ ìŠ¤í¬ë¡¤ + AI ìš”ì•½/Word Export)"""
import warnings
warnings.filterwarnings("ignore")

import os, base64
from io import BytesIO
from pathlib import Path
from math import pi
from itertools import cycle

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# --- ìƒë‹¨ import ê·¼ì²˜ì— ì¶”ê°€ ---
from docx import Document
from docx.shared import Inches, Pt
from docx.oxml.ns import qn

def _apply_korean_fonts(doc, font_name="Malgun Gothic", size_pt=11):
    """
    ë¬¸ì„œ ê¸°ë³¸/ì œëª© ìŠ¤íƒ€ì¼ì— í•œê¸€ í°íŠ¸ë¥¼ ì ìš© (í•œê¸€ ê¹¨ì§ ë°©ì§€).
    - Windows: 'Malgun Gothic'
    - macOS: 'Apple SD Gothic Neo' ë“±ìœ¼ë¡œ ë°”ê¿”ë„ ë¨
    """
    # Normal
    style = doc.styles["Normal"]
    style.font.name = font_name
    style.font.size = Pt(size_pt)
    rpr = style._element.get_or_add_rPr()
    rFonts = rpr.get_or_add_rFonts()
    rFonts.set(qn("w:eastAsia"), font_name)
    rFonts.set(qn("w:ascii"), font_name)
    rFonts.set(qn("w:hAnsi"), font_name)

    # Heading 1~3
    for h in ["Heading 1", "Heading 2", "Heading 3"]:
        if h in doc.styles:
            st_h = doc.styles[h]
            st_h.font.name = font_name
            rpr = st_h._element.get_or_add_rPr()
            rFonts = rpr.get_or_add_rFonts()
            rFonts.set(qn("w:eastAsia"), font_name)
            rFonts.set(qn("w:ascii"), font_name)
            rFonts.set(qn("w:hAnsi"), font_name)

def export_word(
    doc_title: str,
    model: str,
    gpt_analysis_text: str,                 # â† GPTê°€ ë§Œë“  "ë¶„ì„ ê²°ê³¼" ì›ë¬¸
    main_imgs: list[tuple[str, bytes]],     # (ìº¡ì…˜, PNG ë°”ì´íŠ¸)
    profile_imgs: list[tuple[str, bytes]],  # (ìº¡ì…˜, PNG ë°”ì´íŠ¸)
    dfm: pd.DataFrame,
    num_pool: list[str],
    votes: dict,
    k_final: int,
    template_path: Path | None = None,      # â† ì²« ë²ˆì§¸ íŒŒì¼ ìŠ¤íƒ€ì¼ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì“°ê³  ì‹¶ìœ¼ë©´ ì§€ì •
    font_name: str = "Malgun Gothic"
) -> BytesIO:
    """
    Word ë³´ê³ ì„œ ìƒì„±:
    - í‘œì§€/ë©”íƒ€
    - (ì‹ ê·œ) ë¶„ì„ ê²°ê³¼(=GPT ìƒì„±) ì„¹ì…˜
    - ì£¼ìš” ì‹œê°í™”(ì´ë¯¸ì§€)
    - í´ëŸ¬ìŠ¤í„° ìš”ì•½ í†µê³„(í…Œì´ë¸”)
    - ì¶”ê°€ í”„ë¡œíŒŒì¼(ì´ë¯¸ì§€)
    """
    # 1) í…œí”Œë¦¿ ì‚¬ìš© or ë¹ˆ ë¬¸ì„œ
    if template_path and template_path.exists():
        doc = Document(str(template_path))
    else:
        doc = Document()

    # 2) í•œê¸€ í°íŠ¸ ì ìš©
    _apply_korean_fonts(doc, font_name=font_name, size_pt=11)

    # 3) í‘œì§€/ë©”íƒ€
    doc.add_heading(doc_title, level=0)
    doc.add_paragraph(f"ëª¨ë¸: {model}")
    doc.add_paragraph(f"ìµœì¢… k: {k_final}  (Sil={votes.get('silhouette')}, Elbow={votes.get('elbow')}, Dend={votes.get('dendrogram')})")

    # 4) ğŸ” ë¶„ì„ ê²°ê³¼ (GPT ìƒì„±)
    #    â†’ ì²« ë²ˆì§¸ íŒŒì¼ â€œì²˜ëŸ¼â€ ë³´ì´ë„ë¡ ë…ë¦½ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±
    doc.add_heading("ë¶„ì„ ê²°ê³¼ (GPT ìƒì„±)", level=1)
    for para in gpt_analysis_text.split("\n"):
        if para.strip():
            doc.add_paragraph(para.strip())

    # 5) ì£¼ìš” ì‹œê°í™”
    doc.add_heading("ì£¼ìš” ì‹œê°í™”", level=1)
    for cap, png in main_imgs:
        doc.add_paragraph(cap)
        doc.add_picture(BytesIO(png), width=Inches(6.2))

    # 6) í´ëŸ¬ìŠ¤í„° ìš”ì•½ í†µê³„ (í…Œì´ë¸”)
    doc.add_heading("í´ëŸ¬ìŠ¤í„° ìš”ì•½ í†µê³„", level=1)
    counts = dfm["cluster"].value_counts().sort_index()
    means  = dfm.groupby("cluster")[num_pool].mean().round(2)

    tbl = doc.add_table(rows=1, cols=2 + len(num_pool))
    hdr = tbl.rows[0].cells
    hdr[0].text = "Cluster"
    hdr[1].text = "Count"
    for i, c in enumerate(num_pool, start=2):
        hdr[i].text = f"Mean {c}"

    for c in counts.index:
        row = tbl.add_row().cells
        row[0].text = str(c)
        row[1].text = str(int(counts[c]))
        for i, col in enumerate(num_pool, start=2):
            row[i].text = str(means.loc[c, col])

    # 7) ì¶”ê°€ í”„ë¡œíŒŒì¼
    if profile_imgs:
        doc.add_heading("ì¶”ê°€ í”„ë¡œíŒŒì¼", level=1)
        for cap, png in profile_imgs:
            doc.add_paragraph(cap)
            doc.add_picture(BytesIO(png), width=Inches(6.2))

    # 8) ì €ì¥
    bio = BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio
# â”€â”€ ì˜µì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬(ê³„ì‚° ì „ìš©)
try:
    from scipy.cluster.hierarchy import linkage
    _has_scipy = True
except Exception:
    _has_scipy = False

try:
    from yellowbrick.cluster import KElbowVisualizer
    _has_yb = True
except Exception:
    _has_yb = False

# â”€â”€ AI & Word
try:
    from openai import OpenAI
    _has_openai = True
except Exception:
    _has_openai = False

try:
    from docx import Document
    from docx.shared import Inches
    _has_docx = True
except Exception:
    _has_docx = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mpl.rcParams["font.family"] = "DejaVu Sans"
mpl.rcParams["axes.unicode_minus"] = False
st.header("ğŸš— ì°¨ëª…ë³„ K-means êµ°ì§‘ ë¶„ì„")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_PATH = Path("data/SoH_NCM_Dataset_selected_Fid_ë°_ë°°í„°ë¦¬ë“±ê¸‰ì—´ì¶”ê°€.xlsx")
uploaded = st.sidebar.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ(ì„ íƒ)", type=["xlsx"])

def load_excel(path_or_buffer) -> pd.DataFrame:
    df = pd.read_excel(path_or_buffer, engine="openpyxl")
    df.columns = df.columns.map(lambda x: str(x).strip())
    return df

if uploaded:
    df_raw = load_excel(uploaded)
    st.success("ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
elif DATA_PATH.exists():
    df_raw = load_excel(DATA_PATH)
else:
    st.error("ê¸°ë³¸ ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¬ëŸ¼ í‘œì¤€í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    def pick_first(cands):
        for c in cands:
            if c in out.columns: return c
        return None
    mapping = {}
    schema = [
        ("Model",       ["ì°¨ëª…", "ë°°í„°ë¦¬ì¢…ë¥˜", "ì°¨ì¢…", "ëª¨ë¸"]),
        ("Age",         ["ì‚¬ìš©ì—°ìˆ˜(t)", "ì‚¬ìš©ì—°ìˆ˜", "ì—°ì‹"]),
        ("SoH",         ["SoH_pred(%)", "SoH(%)", "SOH"]),
        ("Price",       ["ì¤‘ê³ ê±°ë˜ê°€ê²©", "ê°œë‹¹ê°€ê²©", "ê±°ë˜ê¸ˆì•¡", "ê°€ê²©"]),
        ("CellBalance", ["ì…€ ê°„ ê· í˜•", "ì…€ê°„ê· í˜•"]),
    ]
    for std,cands in schema:
        src = pick_first(cands)
        if src: mapping[src] = std
    out = out.rename(columns=mapping)
    if out.columns.duplicated().any():
        out = out.loc[:, ~out.columns.duplicated()]
    if "CellBalance" in out.columns:
        out["CellBalance"] = (
            out["CellBalance"]
              .map({"ìš°ìˆ˜":"Good","ì •ìƒ":"Normal","ê²½ê³ ":"Warning","ì‹¬ê°":"Critical"})
              .fillna(out["CellBalance"])
        )
    if "Price" in out.columns:
        out["Price"] = (out["Price"].astype(str)
                        .str.replace(r"[^\d.\-]", "", regex=True)
                        .pipe(pd.to_numeric, errors="coerce"))
    if "Age" in out.columns:
        out["Age"] = pd.to_numeric(out["Age"], errors="coerce")
    if "SoH" in out.columns:
        out["SoH"] = pd.to_numeric(out["SoH"], errors="coerce")
    return out

df = normalize_columns(df_raw)

if "Model" not in df.columns:
    st.error("ì—‘ì…€ì— 'ì°¨ëª…/ë°°í„°ë¦¬ì¢…ë¥˜/ì°¨ì¢…/ëª¨ë¸' ì¤‘ í•˜ë‚˜ê°€ ì—†ì–´ Model ì»¬ëŸ¼ì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

num_pool = [c for c in ["Age","SoH","Price"] if c in df.columns]
if len(num_pool) < 2:
    st.error(f"ìˆ˜ì¹˜ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤(í•„ìš”â‰¥2). í˜„ì¬: {num_pool}")
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
models        = sorted(df["Model"].dropna().astype(str).unique())
choice        = st.sidebar.selectbox("ì°¨ëª… ì„ íƒ", models)
show_tsne     = st.sidebar.checkbox("t-SNE 2D ì¶”ê°€", value=True)
show_pca3     = st.sidebar.checkbox("PCA 3D ì¶”ê°€", value=False)
perplexity    = st.sidebar.slider("t-SNE perplexity", 5, 50, 30, 1)
show_profiles = st.sidebar.checkbox("ì¶”ê°€ í”„ë¡œíŒŒì¼(ê°€ë¡œ ìŠ¤í¬ë¡¤)", value=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sub_all = df[df["Model"].astype(str) == str(choice)].copy().dropna(subset=num_pool)
n = len(sub_all)
if n < 3:
    st.warning(f"'{choice}' ìœ íš¨ í‘œë³¸ì´ {n}ê±´ì´ë¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤(â‰¥3 í•„ìš”).")
    st.stop()

ks = list(range(2, min(10, n)))

preproc = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_pool),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"),
         ["CellBalance"] if "CellBalance" in sub_all.columns else []),
    ],
    remainder="drop",
)
X = preproc.fit_transform(sub_all)
if hasattr(X, "toarray"): X = X.toarray()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k = Silhouette + Elbow + Dendrogram â†’ ì¤‘ì•™ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def choose_k_multi(X, ks):
    votes = {}
    try:
        sil_scores = [silhouette_score(X, KMeans(n_clusters=k, random_state=42, n_init="auto")
                          .fit_predict(X)) for k in ks if k < len(X)]
        if sil_scores: votes["silhouette"] = ks[int(np.argmax(sil_scores))]
    except Exception: pass
    try:
        if _has_yb:
            viz = KElbowVisualizer(KMeans(random_state=42), k=ks, metric="distortion", timings=False)
            viz.fit(X); 
            if viz.elbow_value_ is not None: votes["elbow"] = int(viz.elbow_value_)
        else:
            inertias = [KMeans(n_clusters=k, random_state=42, n_init="auto").fit(X).inertia_ for k in ks]
            if len(inertias) >= 2:
                diffs = np.diff(inertias); idx = int(np.argmax(diffs))
                votes["elbow"] = ks[idx+1] if idx+1 < len(ks) else ks[-1]
    except Exception: pass
    try:
        if _has_scipy:
            m = X.shape[0]; idx = np.arange(m if m <= 200 else 200)
            Z = linkage(X[idx], method="ward")
            dists = Z[:,2]; gaps = np.diff(dists)
            if len(gaps) >= 1:
                k_est = m - (int(np.argmax(gaps))+1)
                votes["dendrogram"] = max(2, min(k_est, ks[-1]))
    except Exception: pass
    vals = [v for v in [votes.get("silhouette"), votes.get("elbow"), votes.get("dendrogram")] if v is not None]
    return (int(np.median(vals)) if vals else 3), votes

k_final, votes = choose_k_multi(X, ks)
st.caption(f"ì„ íƒëœ k = {k_final} (Sil={votes.get('silhouette','â€”')}, "
           f"Elbow={votes.get('elbow','â€”')}, Dend={votes.get('dendrogram','â€”')} â†’ median)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•™ìŠµ & ë¼ë²¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
labels = KMeans(n_clusters=k_final, random_state=42, n_init="auto").fit_predict(X)
sub_all = sub_all.copy(); sub_all["cluster"] = labels
clusters = sorted(sub_all["cluster"].unique())

# â”€â”€ ìœ í‹¸: fig png ë³€í™˜ & base64
def fig_to_png(fig, dpi=160):
    buf = BytesIO(); fig.savefig(buf, format="png", dpi=dpi, bbox_inches="tight"); plt.close(fig)
    return buf.getvalue()

def to_b64(png_bytes): return base64.b64encode(png_bytes).decode("utf-8")

# â”€â”€ ê³µí†µ CSS(ê°€ë¡œ ìŠ¤í¬ë¡¤)
st.markdown("""
<style>
.scroll-x { overflow-x:auto; padding:8px 0 10px; }
.scroll-row { display:inline-flex; gap:16px; }
.scroll-row img { border-radius:12px; box-shadow:0 2px 8px rgba(0,0,0,.12); }
.caption-center { text-align:center; color:#6b7280; font-size:12px; }
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ ê·¸ë˜í”„(ê°€ë¡œ ìŠ¤í¬ë¡¤) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
main_images = []  # (caption, png_bytes)

# PCA 2D
p2 = PCA(2, random_state=42).fit_transform(X)
fig = plt.figure(figsize=(5.2, 4.0))
plt.scatter(p2[:,0], p2[:,1], c=labels, cmap="tab10", s=55, edgecolors="k", alpha=0.9)
plt.title(f"{choice}: PCA 2D (k={k_final})"); plt.xlabel("PC1"); plt.ylabel("PC2"); plt.tight_layout()
png = fig_to_png(fig); main_images.append(("PCA 2D", png))

# Radar (í´ëŸ¬ìŠ¤í„° í‰ê· , 0~1 ì •ê·œí™”)
mean_matrix = sub_all.groupby("cluster")[num_pool].mean()
norm_means = mean_matrix.copy()
for c in num_pool:
    mn, mx = df[c].min(), df[c].max()
    norm_means[c] = 0.5 if (pd.isna(mn) or pd.isna(mx) or mx==mn) else (norm_means[c]-mn)/(mx-mn)

angles = [i/len(num_pool)*2*pi for i in range(len(num_pool))] + [0]
fig = plt.figure(figsize=(5.2, 4.0)); ax = plt.subplot(111, polar=True)
for i in clusters:
    vals = norm_means.loc[i].tolist() + [norm_means.loc[i].tolist()[0]]
    ax.plot(angles, vals, label=f"Cluster {i}"); ax.fill(angles, vals, alpha=0.1)
ax.set_xticks(angles[:-1]); ax.set_xticklabels(num_pool)
plt.title(f"{choice}: Radar (k={k_final})"); plt.legend(loc="upper right", bbox_to_anchor=(1.25,1.05))
png = fig_to_png(fig); main_images.append(("Radar", png))

# t-SNE 2D (ì˜µì…˜)
if show_tsne:
    perp = min(perplexity, n-1)
    ts2 = TSNE(n_components=2, perplexity=perp, max_iter=500, random_state=42, init="pca").fit_transform(X)
    fig = plt.figure(figsize=(5.2, 4.0))
    plt.scatter(ts2[:,0], ts2[:,1], c=labels, cmap="tab10", s=55, edgecolors="k", alpha=0.9)
    plt.title(f"{choice}: t-SNE 2D (k={k_final})"); plt.xlabel("t-SNE1"); plt.ylabel("t-SNE2"); plt.tight_layout()
    png = fig_to_png(fig); main_images.append(("t-SNE 2D", png))

# PCA 3D (ì˜µì…˜)
if show_pca3:
    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401
    p3 = PCA(3, random_state=42).fit_transform(X)
    fig = plt.figure(figsize=(5.6, 4.2)); ax3 = fig.add_subplot(111, projection="3d")
    ax3.scatter(p3[:,0], p3[:,1], p3[:,2], c=labels, cmap="tab10", s=50, edgecolors="k", alpha=0.85)
    ax3.set_title(f"{choice}: PCA 3D (k={k_final})"); ax3.set_xlabel("PC1"); ax3.set_ylabel("PC2"); ax3.set_zlabel("PC3")
    png = fig_to_png(fig); main_images.append(("PCA 3D", png))

# í™”ë©´ í‘œì‹œ(ê°€ë¡œ ìŠ¤í¬ë¡¤)
html_imgs = "".join([f"<img src='data:image/png;base64,{to_b64(p)}' height='320'/>" for _,p in main_images])
st.markdown(f"<div class='scroll-x'><div class='scroll-row'>{html_imgs}</div></div>", unsafe_allow_html=True)
st.markdown("<div class='caption-center'>ì¢Œìš° ìŠ¤í¬ë¡¤ë¡œ ê²°ê³¼ ê·¸ë˜í”„(PCA2D, Radar, ì˜µì…˜: t-SNE/PCA3D)ë¥¼ í™•ì¸í•˜ì„¸ìš”.</div>", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶”ê°€ í”„ë¡œíŒŒì¼(ê°€ë¡œ ìŠ¤í¬ë¡¤) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
profile_images = []
if show_profiles:
    # Boxplots
    for col in num_pool:
        fig = plt.figure(figsize=(6,4)); sns.boxplot(x="cluster", y=col, data=sub_all, palette="tab10")
        plt.title(f"{choice}: {col} by Cluster (k={k_final})")
        profile_images.append((f"Box {col}", fig_to_png(fig)))
    # Count & Stacked
    if "CellBalance" in sub_all.columns:
        fig = plt.figure(figsize=(6,4))
        sns.countplot(x="cluster", hue="CellBalance", data=sub_all, palette="Set2")
        plt.title(f"{choice}: Count of CellBalance by Cluster")
        profile_images.append(("Count CellBalance", fig_to_png(fig)))

        ctab_pct = pd.crosstab(sub_all["cluster"], sub_all["CellBalance"], normalize="index")*100
        ctab_pct = ctab_pct.reindex(clusters, fill_value=0)
        fig = plt.figure(figsize=(6,4)); ax = plt.gca()
        ctab_pct.plot(kind="bar", stacked=True, colormap="Paired", ax=ax)
        plt.title(f"{choice}: CellBalance Distribution (%) by Cluster"); plt.tight_layout()
        profile_images.append(("Stacked CellBalance", fig_to_png(fig)))
    # Heatmap
    mean_matrix = sub_all.groupby("cluster")[num_pool].mean()
    fig = plt.figure(figsize=(6,4))
    sns.heatmap(mean_matrix, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title(f"{choice}: Numeric Feature Means per Cluster")
    profile_images.append(("Heatmap Means", fig_to_png(fig)))

    html_prof = "".join([f"<img src='data:image/png;base64,{to_b64(p)}' height='300'/>" for _,p in profile_images])
    st.markdown(f"<div class='scroll-x'><div class='scroll-row'>{html_prof}</div></div>", unsafe_allow_html=True)
    st.markdown("<div class='caption-center'>ì¶”ê°€ í”„ë¡œíŒŒì¼ë„ ê°€ë¡œ ìŠ¤í¬ë¡¤ë¡œ í™•ì¸í•˜ì„¸ìš”.</div>", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ë¶„ì„ê²°ê³¼ & Word ë‚´ë³´ë‚´ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ§  GPT ë¶„ì„ê²°ê³¼ & Word ë‚´ë³´ë‚´ê¸°")

# ì„¸ì…˜ ìƒíƒœ ë³´ê´€(ì¬ì‹¤í–‰ ì‹œ ìœ ì§€)
if "ai_text" not in st.session_state:
    st.session_state.ai_text = None

def build_stats_text(dfm: pd.DataFrame) -> str:
    counts = dfm["cluster"].value_counts().sort_index()
    means  = dfm.groupby("cluster")[num_pool].mean()
    lines = [f"ì´ í‘œë³¸ìˆ˜: {len(dfm)}", f"í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {dfm['cluster'].nunique()}"]
    for c in counts.index:
        part = f"Cluster {c}: ê°œìˆ˜ {int(counts[c])}, " + ", ".join([f"{col} í‰ê·  {means.loc[c, col]:.2f}" for col in num_pool])
        lines.append(part)
    return "\n".join(lines)

def generate_ai_summary(model: str, k_final: int, votes: dict, dfm: pd.DataFrame) -> str:
    stats = build_stats_text(dfm)
    # === ì—¬ê¸°ì„œ ì‹¤ì œ GPT í˜¸ì¶œì„ ë„£ìœ¼ì„¸ìš” (openai ë¼ì´ë¸ŒëŸ¬ë¦¬) ===
    # í™˜ê²½/ë³´ì•ˆ ì‚¬ì •ì— ë”°ë¼ OpenAI API ì—†ì´ ë¡œì»¬ ìš”ì•½ì„ ë°˜í™˜í•˜ë„ë¡ fallback í¬í•¨
    try:
        from openai import OpenAI
        import os
        api_key = st.secrets.get("OPENAI_API_KEY", None) if hasattr(st, "secrets") else None
        api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY not set")

        client = OpenAI(api_key=api_key)
        prompt = f"""
ë‹¹ì‹ ì€ ë°°í„°ë¦¬ ì¤‘ê³ ê±°ë˜ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "ë¶„ì„ ê²°ê³¼" ì„¹ì…˜ì„ í•œêµ­ì–´ë¡œ 350~500ì ë²”ìœ„ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì°¨ì¢…: {model}
- ìµœì¢… k: {k_final} (Sil={votes.get('silhouette')}, Elbow={votes.get('elbow')}, Dend={votes.get('dendrogram')})
- í†µê³„ ìš”ì•½:
{stats}
ìš”êµ¬ì‚¬í•­:
1) êµ°ì§‘ë³„ í•µì‹¬ íŠ¹ì§•(ì—°ì‹/SoH/ê°€ê²© ê²½í–¥)ì„ ë¹„êµ ìš”ì•½
2) ë¦¬ë§ˆì¼€íŒ…/ì •ë¹„/ì¶”ê°€ ì ê²€ ë“± ì‹¤ë¬´ í™œìš© í¬ì¸íŠ¸ 2~3ê°€ì§€
3) ê³¼ë„í•œ ìˆ˜ì‚¬ëŠ” í”¼í•˜ê³  ëª…í™•Â·ê°„ê²°í•˜ê²Œ
"""
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a concise Korean data analyst."},
                {"role": "user",   "content": prompt},
            ],
            temperature=0.2,
        )
        return res.choices[0].message.content.strip()
    except Exception as e:
        # Fallback(ë¡œì»¬ ìƒì„±): GPT ë¯¸ì‚¬ìš© ì‹œ ê°„ë‹¨ ìš”ì•½
        cluster_means = dfm.groupby("cluster")[num_pool].mean().round(1)
        top_price = cluster_means["Price"].idxmax() if "Price" in cluster_means.columns else None
        return (
            f"[ë¡œì»¬ ìš”ì•½] {model}ì— ëŒ€í•´ k={k_final}ë¡œ êµ°ì§‘í™”í–ˆìŠµë‹ˆë‹¤. ì—°ì‹ê³¼ SoHë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµ°ì§‘ ê°„ ìˆ˜ì¤€ì°¨ê°€ í™•ì¸ë˜ë©°, "
            f"ìƒëŒ€ì ìœ¼ë¡œ SoHê°€ ë†’ê³  ê°€ê²© ìˆ˜ì¤€ì´ ë†’ì€ êµ°ì§‘({top_price}ë²ˆ)ì€ ë¦¬ë§ˆì¼€íŒ… íƒ€ê¹ƒìœ¼ë¡œ, ì €SoH êµ°ì§‘ì€ ì„±ëŠ¥ì ê²€/ì •ë¹„ ê¶Œê³ ê°€ ìœ íš¨í•©ë‹ˆë‹¤."
        )

col_a, col_b = st.columns([1,2])
with col_a:
    gen_btn = st.button("ğŸ§  ë¶„ì„ê²°ê³¼ ìƒì„± & Wordë¡œ ì €ì¥", use_container_width=True)

with col_b:
    if st.session_state.ai_text:
        st.markdown("**ğŸ” ë¶„ì„ ê²°ê³¼ (GPT ìƒì„±)**")
        st.write(st.session_state.ai_text)

if gen_btn:
    with st.spinner("GPT ë¶„ì„ê²°ê³¼ ìƒì„± ë° Word ë¬¸ì„œ ì‘ì„± ì¤‘..."):
        # 1) GPT ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±
        ai_text = generate_ai_summary(choice, k_final, votes, sub_all)
        st.session_state.ai_text = ai_text  # í™”ë©´ì—ë„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì €ì¥

        # 2) Word ë‚´ë³´ë‚´ê¸°
        #    ì²« ë²ˆì§¸ íŒŒì¼(ì˜ˆ: /mnt/data/EV_Battery_Report_Full.docx)ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì“°ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ê²½ë¡œ ì§€ì •
        TEMPLATE_PATH = Path("data/EV_Battery_Report_Full.docx")  # ì—†ìœ¼ë©´ Noneë¡œ ë‘¬ë„ OK
        template_to_use = TEMPLATE_PATH if TEMPLATE_PATH.exists() else None

        word_buf = export_word(
            doc_title=f"EV ë°°í„°ë¦¬ êµ°ì§‘ ë¶„ì„ ë³´ê³ ì„œ â€“ {choice}",
            model=choice,
            gpt_analysis_text=ai_text,              # â† GPT ë¶„ì„ê²°ê³¼ë¥¼ ë¬¸ì„œì— ì‚½ì…
            main_imgs=main_images,
            profile_imgs=profile_images if show_profiles else [],
            dfm=sub_all,
            num_pool=num_pool,
            votes=votes,
            k_final=k_final,
            template_path=template_to_use,          # â† í…œí”Œë¦¿ ìˆìœ¼ë©´ ìŠ¤íƒ€ì¼ ìƒì†
            font_name="Malgun Gothic"               # â† í•œê¸€ ê¹¨ì§ ë°©ì§€
        )
    st.success("ë³´ê³ ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    st.download_button(
        "â¬‡ï¸ Word íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=word_buf,
        file_name=f"EV_Battery_Report_{choice}.docx",
        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        use_container_width=True,
    )

