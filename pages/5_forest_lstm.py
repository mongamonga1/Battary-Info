# -*- coding: utf-8 -*-
"""Forest LSTM (Streamlit í˜ì´ì§€ìš© Â· Colab ë§¤ì§ ì œê±°)"""

import os, re, io, sys, math, json, textwrap, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from pathlib import Path                       # ğŸ”¸ ì¶”ê°€

# PyTorch (LSTMìš©) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
try:
    import torch
    from torch import nn
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False
# â”€â”€ ê²½ëŸ‰ í…Œë§ˆ(ìƒ‰ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apply_colors(page_bg="#F5F7FB", sidebar_bg="#0F172A", sidebar_text="#DBE4FF", sidebar_link="#93C5FD"):
    st.markdown(f"""
    <style>
      .stApp {{ background: {page_bg}; }}
      section[data-testid="stSidebar"] {{ background: {sidebar_bg}; }}
      section[data-testid="stSidebar"] * {{ color: {sidebar_text} !important; }}
      section[data-testid="stSidebar"] a, section[data-testid="stSidebar"] svg {{
        color: {sidebar_link} !important; fill: {sidebar_link} !important;
      }}
      section[data-testid="stSidebar"] a:hover {{ background-color: rgba(255,255,255,0.08) !important; border-radius: 8px; }}
    </style>
    """, unsafe_allow_html=True)

apply_colors(
    page_bg="#F5F7FB",
    sidebar_bg="#0F172A",
    sidebar_text="#FFFFFF",
    sidebar_link="#93C5FD"
)
st.markdown("""
<style>
/* (1) ë“œë¡­ì¡´ ë°•ìŠ¤ */
section[data-testid="stSidebar"] [data-testid*="FileUploaderDropzone"]{
  background-color:#1E293B !important;
  border:1.5px dashed #94A3B8 !important;
  border-radius:12px !important;
}

/* (2) í˜¸í™˜ìš©(ê¸°ì¡´ í´ë˜ìŠ¤ ê²½ë¡œ) */
section[data-testid="stSidebar"] .stFileUploader [data-testid*="FileUploaderDropzone"],
section[data-testid="stSidebar"] .stFileUploader > div > div{
  background-color:#1E293B !important;
  border:1.5px dashed #94A3B8 !important;
  border-radius:12px !important;
}

/* (3) ë“œë¡­ì¡´ ë‚´ë¶€ ì•ˆë‚´ë¬¸ í…ìŠ¤íŠ¸ë§Œ ë°ê²Œ â€” 'ë²„íŠ¼'ì€ ì œì™¸ */
section[data-testid="stSidebar"] [data-testid*="FileUploaderDropzone"] *:not(button):not([role="button"]):not(button *):not([role="button"] *),
section[data-testid="stSidebar"] .stFileUploader [data-testid*="FileUploaderDropzone"] *:not(button):not([role="button"]):not(button *):not([role="button"] *){
  color:#EAF2FF !important;
  opacity:1 !important;
  filter:none !important;
}

/* (4) ì—…ë¡œë”ì˜ â€˜Browse filesâ€™ ë²„íŠ¼(ë° ë¼ë²¨)ë§Œ ì§„í•˜ê²Œ */
section[data-testid="stSidebar"] [data-testid*="FileUploader"] button,
section[data-testid="stSidebar"] [data-testid*="FileUploader"] [role="button"],
section[data-testid="stSidebar"] [data-testid*="FileUploader"] button *,
section[data-testid="stSidebar"] [data-testid*="FileUploader"] [role="button"] *{
  background-color:#F1F5F9 !important;
  color:#0F172A !important;
  font-weight:700 !important;
  opacity:1 !important;
}
/* ì‚¬ì´ë“œë°” selectbox(ì…ë ¥ì°½) í…ìŠ¤íŠ¸ë§Œ ê²€ì • */
section[data-testid="stSidebar"] [data-testid="stSelectbox"] [data-baseweb="select"] *{
  color:#0F172A !important;
}

/* (ì˜µì…˜) í¼ì³ì§„ ì˜µì…˜ ëª©ë¡ í…ìŠ¤íŠ¸ë„ ê²€ì • */
div[data-baseweb="popover"] [data-baseweb="menu"] *{
  color:#0F172A !important;
}
</style>
""", unsafe_allow_html=True)
# ----------------------------- 1) ì„¤ì •ê°’ -----------------------------
SEED = 42
np.random.seed(SEED)

CUTOFF_DATE = pd.Timestamp("2025-07-27")
CONTAMINATION = 0.02
RUN_LSTM = True
MIN_TXN_FOR_LSTM = 40
SEQ_LEN = 12
LSTM_EPOCHS = 40
LSTM_LR = 1e-3
LSTM_BATCH = 64
LSTM_HIDDEN = 16
LSTM_LAYERS = 1

# ğŸ”¸ Colab ê²½ë¡œ â†’ í”„ë¡œì íŠ¸ ë‚´ë¶€ ìƒëŒ€ ê²½ë¡œ
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# ----------------------------- 2) ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def parse_money(x):
    if pd.isna(x):
        return np.nan
    s = re.sub(r"[^\d\-]", "", str(x))
    return float(s) if s else np.nan

def to_datetime(x):
    try:
        return pd.to_datetime(x)
    except Exception:
        return pd.NaT

def minmax(arr):
    mn, mx = np.nanmin(arr), np.nanmax(arr)
    if not np.isfinite(mn) or not np.isfinite(mx) or mx - mn == 0:
        return np.zeros_like(arr, dtype=float)
    return (arr - mn) / (mx - mn)

def add_expanding_stats(df, group_cols, target, prefix):
    df = df.copy()
    grp = df.groupby(group_cols, group_keys=False)
    df[f"{prefix}_mean"] = grp[target].apply(lambda s: s.shift().expanding().mean())
    df[f"{prefix}_std"]  = grp[target].apply(lambda s: s.shift().expanding().std())
    df[f"{prefix}_cnt"]  = grp[target].apply(lambda s: s.shift().expanding().count())
    return df

def z_from(val, mean, std):
    std = std.replace(0, np.nan)
    return (val - mean) / std

def prior_partner_count(df_in):
    df_in = df_in.sort_values("ê³„ì•½ì¼")
    out = pd.Series(index=df_in.index, dtype=float)
    for _, g in df_in.groupby("íŒë§¤ì—…ì²´"):
        seen = set()
        counts = []
        for _, row in g.iterrows():
            counts.append(len(seen))
            seen.add(row["êµ¬ë§¤ì—…ì²´"])
        out.loc[g.index] = counts
    return out

# ----------------------------- 3) LSTM ì˜¤í† ì¸ì½”ë” ì •ì˜ -----------------------------
class LSTMAE(nn.Module):
    def __init__(self, input_dim, hidden_size=16, num_layers=1):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_size, num_layers=num_layers, batch_first=True)
        self.decoder = nn.LSTM(hidden_size, input_dim, num_layers=num_layers, batch_first=True)
    def forward(self, x):
        z, _ = self.encoder(x)
        last = z[:, -1:, :]
        last_rep = last.repeat(1, x.size(1), 1)
        out, _ = self.decoder(last_rep)
        return out

def make_sequences(arr, seq_len):
    return np.stack([arr[i:i+seq_len] for i in range(len(arr)-seq_len+1)])

# ----------------------------- 4) CSV ë¡œë“œ -----------------------------
DATA_PATH = Path("data/í†µí•©ê±°ë˜ë‚´ì—­.csv")
df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
print(f"ì½ì€ íŒŒì¼: {DATA_PATH.name}, shape={df.shape}")

# ----------------------------- 5) ì •ì œ -----------------------------
for col in ["ê°œë‹¹ê°€ê²©", "ì´ê³„ì•½ê¸ˆì•¡", "ê³„ì•½ë³´ì¦ê¸ˆ"]:
    if col in df.columns:
        df[col] = (
            df[col].astype(str)
                  .str.replace(r"[^\d\-]", "", regex=True)
                  .replace("", np.nan)
                  .astype(float)
        )

money_cols = ["ê°œë‹¹ ê°€ê²©", "ì´ê³„ì•½ê¸ˆì•¡", "ê³„ì•½ë³´ì¦ê¸ˆ"]
for c in money_cols:
    if c in df.columns:
        df[c] = df[c].apply(parse_money)

date_cols = ["ê³„ì•½ì¼", "ì´ì§€ê¸‰ì¼"]
for c in date_cols:
    if c in df.columns:
        df[c] = df[c].apply(to_datetime)

df["ì§€ê¸‰ì§€ì—°ì¼ìˆ˜"] = (df["ì´ì§€ê¸‰ì¼"] - df["ê³„ì•½ì¼"]).dt.days
df["ë³´ì¦ê¸ˆìœ¨"] = df["ê³„ì•½ë³´ì¦ê¸ˆ"] / df["ì´ê³„ì•½ê¸ˆì•¡"]
df["ì´ê³„ì•½ë‹¨ê°€"] = df["ì´ê³„ì•½ê¸ˆì•¡"] / df["ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)"].replace(0, np.nan)

df = df.sort_values("ê³„ì•½ì¼").reset_index(drop=True)
train_mask = df["ê³„ì•½ì¼"] <= CUTOFF_DATE
df_train = df[train_mask].copy()

# ----------------------------- 6) í”¼ì²˜ë§ -----------------------------
df_feat = df.copy()
cat_cols = ["íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´", "ì œí’ˆêµ¬ë¶„", "ë°°í„°ë¦¬ì¢…ë¥˜", "ì§€ê¸‰í˜•íƒœ"]
for c in cat_cols:
    freq = df_train[c].value_counts()
    df_feat[f"{c}_freq"] = df_feat[c].map(freq).fillna(0)

df_feat["key_íŒë§¤êµ¬ë§¤"] = df_feat["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" + df_feat["êµ¬ë§¤ì—…ì²´"].astype(str)
df_feat["key_íŒë§¤ì œí’ˆë°°í„°ë¦¬"] = (
    df_feat["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" +
    df_feat["ì œí’ˆêµ¬ë¶„"].astype(str) + "âˆ¥" +
    df_feat["ë°°í„°ë¦¬ì¢…ë¥˜"].astype(str)
)

seen_íŒë§¤êµ¬ë§¤ = set(df_train["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" + df_train["êµ¬ë§¤ì—…ì²´"].astype(str))
seen_íŒë§¤ì œí’ˆë°°í„°ë¦¬ = set(
    df_train["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" +
    df_train["ì œí’ˆêµ¬ë¶„"].astype(str) + "âˆ¥" +
    df_train["ë°°í„°ë¦¬ì¢…ë¥˜"].astype(str)
)
df_feat["ì‹ ê·œ_íŒë§¤êµ¬ë§¤"] = (~df_feat["key_íŒë§¤êµ¬ë§¤"].isin(seen_íŒë§¤êµ¬ë§¤)).astype(int)
df_feat["ì‹ ê·œ_íŒë§¤ì œí’ˆë°°í„°ë¦¬"] = (~df_feat["key_íŒë§¤ì œí’ˆë°°í„°ë¦¬"].isin(seen_íŒë§¤ì œí’ˆë°°í„°ë¦¬)).astype(int)

for tcol, pfx in [
    ("ê°œë‹¹ê°€ê²©",        "íŒë§¤ì—…ì²´_unit_price"),
    ("ì´ê³„ì•½ê¸ˆì•¡",      "íŒë§¤ì—…ì²´_total_amount"),
    ("ë³´ì¦ê¸ˆìœ¨",       "íŒë§¤ì—…ì²´_deposit_rate"),
    ("ì§€ê¸‰ì§€ì—°ì¼ìˆ˜",    "íŒë§¤ì—…ì²´_delay_days"),
    ("ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)", "íŒë§¤ì—…ì²´_qty"),
]:
    if tcol in df_feat.columns:
        df_feat = add_expanding_stats(df_feat, ["íŒë§¤ì—…ì²´"], tcol, pfx)

df_feat["z_unit_price"]   = z_from(df_feat["ê°œë‹¹ê°€ê²©"], df_feat["íŒë§¤ì—…ì²´_unit_price_mean"], df_feat["íŒë§¤ì—…ì²´_unit_price_std"])
df_feat["z_total_amount"] = z_from(df_feat["ì´ê³„ì•½ê¸ˆì•¡"], df_feat["íŒë§¤ì—…ì²´_total_amount_mean"], df_feat["íŒë§¤ì—…ì²´_total_amount_std"])
df_feat["z_deposit_rate"] = z_from(df_feat["ë³´ì¦ê¸ˆìœ¨"], df_feat["íŒë§¤ì—…ì²´_deposit_rate_mean"], df_feat["íŒë§¤ì—…ì²´_deposit_rate_std"])
df_feat["z_delay_days"]   = z_from(df_feat["ì§€ê¸‰ì§€ì—°ì¼ìˆ˜"], df_feat["íŒë§¤ì—…ì²´_delay_days_mean"], df_feat["íŒë§¤ì—…ì²´_delay_days_std"])
df_feat["z_qty"]          = z_from(df_feat["ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)"], df_feat["íŒë§¤ì—…ì²´_qty_mean"], df_feat["íŒë§¤ì—…ì²´_qty_std"])

df_feat["íŒë§¤ì—…ì²´_prior_partner_cnt"] = prior_partner_count(df_feat[["íŒë§¤ì—…ì²´","êµ¬ë§¤ì—…ì²´","ê³„ì•½ì¼"]].copy())

feature_cols = [
    "ê°œë‹¹ê°€ê²©","ì´ê³„ì•½ê¸ˆì•¡","ê³„ì•½ë³´ì¦ê¸ˆ","ì§€ê¸‰ì§€ì—°ì¼ìˆ˜","ë³´ì¦ê¸ˆìœ¨","ì´ê³„ì•½ë‹¨ê°€","ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)",
    "íŒë§¤ì—…ì²´_freq","êµ¬ë§¤ì—…ì²´_freq","ì œí’ˆêµ¬ë¶„_freq","ë°°í„°ë¦¬ì¢…ë¥˜_freq","ì§€ê¸‰ í˜•íƒœ_freq",
    "ì‹ ê·œ_íŒë§¤êµ¬ë§¤","ì‹ ê·œ_íŒë§¤ì œí’ˆë°°í„°ë¦¬",
    "z_unit_price","z_total_amount","z_deposit_rate","z_delay_days","z_qty",
    "íŒë§¤ì—…ì²´_unit_price_cnt","íŒë§¤ì—…ì²´_total_amount_cnt","íŒë§¤ì—…ì²´_deposit_rate_cnt","íŒë§¤ì—…ì²´_delay_days_cnt","íŒë§¤ì—…ì²´_qty_cnt",
    "íŒë§¤ì—…ì²´_prior_partner_cnt",
]
for c in feature_cols:
    if c not in df_feat.columns:
        df_feat[c] = 0.0

X_train = df_feat.loc[train_mask, feature_cols].astype(float).fillna(0.0).values
X_all   = df_feat[feature_cols].astype(float).fillna(0.0).values

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_all_s   = scaler.transform(X_all)

# ----------------------------- 7) Isolation Forest -----------------------------
if_model = IsolationForest(
    n_estimators=500,
    contamination=CONTAMINATION,
    max_samples="auto",
    bootstrap=True,
    random_state=SEED,
    n_jobs=1
)
if_model.fit(X_train_s)
if_scores = -if_model.decision_function(X_all_s)
df_feat["if_score"] = if_scores
df_feat["if_score_norm"] = minmax(if_scores)

# ----------------------------- 8) (ì„ íƒ) LSTM ë³´ì¡° ìŠ¤ì½”ì–´ -----------------------------
df_feat["lstm_score_norm"] = np.nan
lstm_rows = []

if RUN_LSTM and TORCH_AVAILABLE:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    lstm_feature_cols = ["ê°œë‹¹ê°€ê²©","ë³´ì¦ê¸ˆìœ¨","ì§€ê¸‰ì§€ì—°ì¼ìˆ˜","ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)","ì´ê³„ì•½ê¸ˆì•¡"]

    counts = df_train["íŒë§¤ì—…ì²´"].value_counts()
    target_vendors = counts[counts >= MIN_TXN_FOR_LSTM].index.tolist()

    class _LSTMAE(nn.Module):
        def __init__(self, input_dim):
            super().__init__()
            self.model = LSTMAE(input_dim, hidden_size=LSTM_HIDDEN, num_layers=LSTM_LAYERS)
        def forward(self, x):
            return self.model(x)

    for vendor in target_vendors:
        g = df_feat[df_feat["íŒë§¤ì—…ì²´"] == vendor].sort_values("ê³„ì•½ì¼").copy()
        g_train = g[g["ê³„ì•½ì¼"] <= CUTOFF_DATE].copy()
        if g.shape[0] < SEQ_LEN + 1 or g_train.shape[0] < SEQ_LEN + 1:
            continue

        scaler_v = StandardScaler()
        g_train_feat = scaler_v.fit_transform(g_train[lstm_feature_cols].astype(float).fillna(0.0).values)
        g_all_feat   = scaler_v.transform(g[lstm_feature_cols].astype(float).fillna(0.0).values)

        train_seqs = make_sequences(g_train_feat, SEQ_LEN)
        all_seqs   = make_sequences(g_all_feat, SEQ_LEN)

        Xtr = torch.tensor(train_seqs, dtype=torch.float32).to(device)
        Xall = torch.tensor(all_seqs, dtype=torch.float32).to(device)

        model = _LSTMAE(len(lstm_feature_cols)).to(device)
        opt = torch.optim.Adam(model.parameters(), lr=LSTM_LR)
        loss_fn = nn.MSELoss()

        model.train()
        for _ in range(LSTM_EPOCHS):
            idx = torch.randperm(Xtr.size(0))
            for i in range(0, Xtr.size(0), LSTM_BATCH):
                sel = idx[i:i+LSTM_BATCH]
                batch = Xtr[sel]
                opt.zero_grad()
                recon = model(batch)
                loss = loss_fn(recon, batch)
                loss.backward()
                opt.step()

        model.eval()
        with torch.no_grad():
            tr_loss = ((model(Xtr) - Xtr) ** 2).mean(dim=(1,2)).cpu().numpy()
            all_loss = ((model(Xall) - Xall) ** 2).mean(dim=(1,2)).cpu().numpy()

        idx_all = g.index.to_list()
        target_row_indices = idx_all[SEQ_LEN-1:]

        p50, p99 = np.percentile(tr_loss, 50), np.percentile(tr_loss, 99)
        denom = max(p99 - p50, 1e-6)
        norm_scores = np.clip((all_loss - p50) / denom, 0, 5) / 5.0

        tmp = pd.DataFrame({
            "íŒë§¤ì—…ì²´": vendor,
            "row_index": target_row_indices,
            "lstm_loss": all_loss,
            "lstm_score_norm": norm_scores
        })
        lstm_rows.append(tmp)

    if lstm_rows:
        lstm_df = pd.concat(lstm_rows, ignore_index=True)
        df_feat.loc[lstm_df["row_index"].values, "lstm_score_norm"] = lstm_df["lstm_score_norm"].values
else:
    lstm_df = pd.DataFrame(columns=["row_index","lstm_loss","lstm_score_norm","íŒë§¤ì—…ì²´"])

# ----------------------------- 9) ìµœì¢… ì ìˆ˜ ê²°í•© -----------------------------
def combine_scores(a, b):
    if pd.isna(b):
        return a
    return max(0.6*a, 0.4*b)

df_feat["final_score"] = [combine_scores(a, b) for a, b in zip(df_feat["if_score_norm"], df_feat["lstm_score_norm"])]

import streamlit as st
import plotly.express as px

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10) Plotly ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
threshold = df_feat["final_score"].quantile(1 - CONTAMINATION)
df_feat["anomaly"] = df_feat["final_score"] >= threshold

fig_anom = px.scatter(
    df_feat,
    x="ê³„ì•½ì¼", y="final_score",
    color="anomaly",
    color_discrete_map={False: "lightblue", True: "red"},
    title=f"Anomalies (top {int(CONTAMINATION*100)}%) Highlighted",
    labels={"ê³„ì•½ì¼":"Contract Date", "final_score":"Final Score", "anomaly":"Anomaly"},
    hover_data={
        "ê³„ì•½ë²ˆí˜¸": True, "íŒë§¤ì—…ì²´": True, "êµ¬ë§¤ì—…ì²´": True,
        "ì œí’ˆêµ¬ë¶„": True, "ë°°í„°ë¦¬ì¢…ë¥˜": True, "final_score": ":.4f"
    }
)
fig_anom.update_traces(marker=dict(size=8))

# Streamlitì— Plotly ì°¨íŠ¸ë¡œ í‘œì‹œ
st.subheader("ğŸ” ì´ìƒì¹˜ ìŠ¤ì½”ì–´ ì‚°ì ë„")
st.plotly_chart(fig_anom, use_container_width=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 11) ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
anom_df = df_feat[df_feat["final_score"] >= threshold]
cols = ["ê³„ì•½ë²ˆí˜¸", "ê³„ì•½ì¼", "íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´", "ì œí’ˆêµ¬ë¶„", "ë°°í„°ë¦¬ì¢…ë¥˜", "final_score"]
top_anom = anom_df[cols].sort_values("final_score", ascending=False)

st.subheader(f"ğŸš¨ Top {int(CONTAMINATION*100)}% ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸ ({len(top_anom)}ê±´)")
st.dataframe(top_anom, use_container_width=True)
