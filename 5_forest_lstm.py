# -*- coding: utf-8 -*-
"""Forest LSTM (Streamlit í˜ì´ì§€ìš© Â· ê²½ëŸ‰í™”/ì•ˆì •í™” íŒ¨ì¹˜, ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶ˆë³€)"""

import os, re, io, sys, math, json, textwrap, warnings
warnings.filterwarnings("ignore")

# â¬‡ï¸ ì†Œí˜• CPUì—ì„œ ê³¼ë„í•œ ìŠ¤ë ˆë”© ë°©ì§€(ì†ë„/ì•ˆì •ì„± â†‘)
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from pathlib import Path
import streamlit as st
import plotly.express as px

# PyTorch (LSTMìš©) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
try:
    import torch
    from torch import nn
    TORCH_AVAILABLE = True
    # â¬‡ï¸ CPU ìŠ¤ë ˆë“œ 1ê°œë¡œ ê³ ì •(ì‘ì€ ë¨¸ì‹ ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ â†“)
    try:
        torch.set_num_threads(1)
    except Exception:
        pass
except Exception:
    TORCH_AVAILABLE = False

# â”€â”€ ê²½ëŸ‰ í…Œë§ˆ(ìƒ‰ìƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apply_colors(page_bg="#F5F7FB", sidebar_bg="#0F172A", sidebar_text="#DBE4FF", sidebar_link="#93C5FD"):
    st.markdown(f"""
    <style>
      .stApp {{ background: {page_bg}; }}
      section[data-testid="stSidebar"] {{ background: {sidebar_bg}; }}
      section[data-testid="stSidebar"] * {{ color: {sidebar_text} !important; }}
      section[data-testid="stSidebar"] a, section[data-testid="stSidebar"] svg {{
        color: {sidebar_link} !important; fill: {sidebar_link} !important;
      }}
      section[data-testid="stSidebar"] a:hover {{ background-color: rgba(255,255,255,0.08) !important; border-radius: 8px; }}
    </style>
    """, unsafe_allow_html=True)

apply_colors(
    page_bg="#F5F7FB",
    sidebar_bg="#0F172A",
    sidebar_text="#FFFFFF",
    sidebar_link="#93C5FD"
)
st.markdown("""
<style>
/* (1) ë“œë¡­ì¡´ ë°•ìŠ¤ */
section[data-testid="stSidebar"] [data-testid*="FileUploaderDropzone"]{
  background-color:#1E293B !important;
  border:1.5px dashed #94A3B8 !important;
  border-radius:12px !important;
}
/* (2) í˜¸í™˜ìš©(ê¸°ì¡´ í´ë˜ìŠ¤ ê²½ë¡œ) */
section[data-testid="stSidebar"] .stFileUploader [data-testid*="FileUploaderDropzone"],
section[data-testid="stSidebar"] .stFileUploader > div > div{
  background-color:#1E293B !important;
  border:1.5px dashed #94A3B8 !important;
  border-radius:12px !important;
}
/* (3) ë“œë¡­ì¡´ ë‚´ë¶€ ì•ˆë‚´ë¬¸ í…ìŠ¤íŠ¸ë§Œ ë°ê²Œ â€” 'ë²„íŠ¼'ì€ ì œì™¸ */
section[data-testid="stSidebar"] [data-testid*="FileUploaderDropzone"] *:not(button):not([role="button"]):not(button *):not([role="button"] *),
section[data-testid="stSidebar"] .stFileUploader [data-testid*="FileUploaderDropzone"] *:not(button):not([role="button"]):not(button *):not([role="button"] *){
  color:#EAF2FF !important;
  opacity:1 !important;
  filter:none !important;
}
/* (4) ì—…ë¡œë”ì˜ â€˜Browse filesâ€™ ë²„íŠ¼(ë° ë¼ë²¨)ë§Œ ì§„í•˜ê²Œ */
section[data-testid="stSidebar"] [data-testid*="FileUploader"] button,
section[data-testid="stSidebar"] [data-testid*="FileUploader"] [role="button"],
section[data-testid="stSidebar"] [data-testid*="FileUploader"] button *,
section[data-testid="stSidebar"] [data-testid*="FileUploader"] [role="button"] *{
  background-color:#F1F5F9 !important;
  color:#0F172A !important;
  font-weight:700 !important;
  opacity:1 !important;
}
/* ì‚¬ì´ë“œë°” selectbox(ì…ë ¥ì°½) í…ìŠ¤íŠ¸ë§Œ ê²€ì • */
section[data-testid="stSidebar"] [data-testid="stSelectbox"] [data-baseweb="select"] *{
  color:#0F172A !important;
}
/* (ì˜µì…˜) í¼ì³ì§„ ì˜µì…˜ ëª©ë¡ í…ìŠ¤íŠ¸ë„ ê²€ì • */
div[data-baseweb="popover"] [data-baseweb="menu"] *{
  color:#0F172A !important;
}
</style>
""", unsafe_allow_html=True)

# ----------------------------- 1) ì„¤ì •ê°’ -----------------------------
SEED = 42
np.random.seed(SEED)

CUTOFF_DATE = pd.Timestamp("2025-07-27")
CONTAMINATION = 0.02
RUN_LSTM = True
MIN_TXN_FOR_LSTM = 40
SEQ_LEN = 12
LSTM_EPOCHS = 40
LSTM_LR = 1e-3
LSTM_BATCH = 64
LSTM_HIDDEN = 16
LSTM_LAYERS = 1

# ğŸ”¸ í”„ë¡œì íŠ¸ ìƒëŒ€ ê²½ë¡œ
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# ----------------------------- 2) ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def parse_money(x):
    if pd.isna(x):
        return np.nan
    s = re.sub(r"[^\d\-]", "", str(x))
    return float(s) if s else np.nan

def to_datetime(x):
    try:
        return pd.to_datetime(x)
    except Exception:
        return pd.NaT

def minmax(arr):
    mn, mx = np.nanmin(arr), np.nanmax(arr)
    if not np.isfinite(mn) or not np.isfinite(mx) or mx - mn == 0:
        return np.zeros_like(arr, dtype=float)
    return (arr - mn) / (mx - mn)

def add_expanding_stats(df, group_cols, target, prefix):
    """
    ë²¡í„°í™”ëœ 'ê³¼ê±°ê¹Œì§€ì˜' ëˆ„ì  í†µê³„:
      - mean/std/count (í˜„ì¬ rowëŠ” ì œì™¸: shift(1) íš¨ê³¼)
    Python ë£¨í”„ ì—†ì´ groupby + cumsumìœ¼ë¡œ ê³„ì‚°í•´ ì†ë„ ê°œì„ .
    """
    out = df.copy()
    # ìˆ«ìí˜•ìœ¼ë¡œ ìºìŠ¤íŒ…
    x = out[target].astype(float)
    # í˜„ì¬í–‰ ì œì™¸(ì‚¬ì „ ì •ë³´ë§Œ ì‚¬ìš©)
    # x_shiftëŠ” out[target]ì„ groupë³„ë¡œ í•œ ì¹¸ ë°€ì–´ë‚¸ ê°’
    x_shift = out.groupby(group_cols)[target].shift(1).astype(float)

    # ëˆ„ì í•©(sum1), ì œê³± ëˆ„ì í•©(sum2), ì¹´ìš´íŠ¸(n)
    tmp = out.copy()
    tmp["_xs"] = x_shift.fillna(0.0)
    tmp["_xs2"] = (x_shift ** 2).fillna(0.0)
    sum1 = tmp.groupby(group_cols)["_xs"].cumsum()
    sum2 = tmp.groupby(group_cols)["_xs2"].cumsum()
    n = tmp.groupby(group_cols).cumcount()  # ê³¼ê±° ê±´ìˆ˜(í˜„ì¬ ì œì™¸)

    # í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°(ìƒ˜í”Œ í‘œì¤€í¸ì°¨, n-1 ë¶„ëª¨)
    mean = sum1 / n.replace(0, np.nan)
    var = (sum2 - (sum1 ** 2) / n.replace(0, np.nan)) / (n - 1).replace({0: np.nan})
    std = np.sqrt(var)

    out[f"{prefix}_mean"] = mean.values
    out[f"{prefix}_std"] = std.values
    out[f"{prefix}_cnt"] = n.values.astype(float)

    # í´ë¦°ì—…
    out.drop(columns=[c for c in ["_xs", "_xs2"] if c in out.columns], inplace=True, errors="ignore")
    return out

def z_from(val, mean, std):
    std = std.replace(0, np.nan)
    return (val - mean) / std

def prior_partner_count(df_in):
    """
    íŒë§¤ì—…ì²´ë³„ë¡œ ê³„ì•½ì¼ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ì‹œ,
    í•´ë‹¹ ì‹œì  ì´ì „ê¹Œì§€ ê³ ìœ  êµ¬ë§¤ì—…ì²´ ìˆ˜(ë²¡í„°í™”).
    """
    d = df_in[["íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´", "ê³„ì•½ì¼"]].copy()
    d = d.sort_values(["íŒë§¤ì—…ì²´", "ê³„ì•½ì¼", "êµ¬ë§¤ì—…ì²´"])
    # (íŒë§¤ì—…ì²´, êµ¬ë§¤ì—…ì²´) ìŒì˜ ì²« ë“±ì¥ í‘œì‹œ
    first_pair = ~d[["íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´"]].duplicated(keep="first")
    # ì—…ì²´ë³„ ê³ ìœ  êµ¬ë§¤ì—…ì²´ ëˆ„ì  ìˆ˜
    cum_unique = first_pair.groupby(d["íŒë§¤ì—…ì²´"]).cumsum()
    # í˜„ì¬ í–‰ ì´ì „ì˜ ê³ ìœ  ìˆ˜ = ëˆ„ì  - (í˜„ì¬ê°€ ì²« ë“±ì¥ì¸ì§€)
    prior = (cum_unique - first_pair.astype(int)).astype(float)
    out = pd.Series(index=df_in.index, dtype=float)
    out.loc[d.index] = prior.values
    return out

# ----------------------------- 3) LSTM ì˜¤í† ì¸ì½”ë” ì •ì˜ -----------------------------
class LSTMAE(nn.Module):
    def __init__(self, input_dim, hidden_size=16, num_layers=1):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_size, num_layers=num_layers, batch_first=True)
        self.decoder = nn.LSTM(hidden_size, input_dim, num_layers=num_layers, batch_first=True)
    def forward(self, x):
        z, _ = self.encoder(x)
        last = z[:, -1:, :]
        last_rep = last.repeat(1, x.size(1), 1)
        out, _ = self.decoder(last_rep)
        return out

def make_sequences(arr, seq_len):
    # (N, F) â†’ (N-seq_len+1, seq_len, F)
    if len(arr) < seq_len:
        return np.empty((0, seq_len, arr.shape[1]), dtype=arr.dtype)
    # ë²¡í„°í™”ëœ ìŠ¬ë¼ì´ì‹±
    idx = np.arange(seq_len)[None, :] + np.arange(len(arr) - seq_len + 1)[:, None]
    return arr[idx]

# ----------------------------- 4) CSV ë¡œë“œ (ìºì‹œ + PyArrow ì•ˆì „ í´ë°±) -----------------------------
DATA_PATH = Path("data/í†µí•©ê±°ë˜ë‚´ì—­.csv")

def _file_sig(path: Path):
    try:
        stt = path.stat()
        return (str(path), stt.st_size, int(stt.st_mtime))
    except FileNotFoundError:
        return (str(path), None, None)

@st.cache_data(show_spinner=False)
def read_csv_fast(path: Path, encoding="utf-8-sig", arrow=True, memory_map=True, low_memory=False):
    """
    - ë¨¼ì € pyarrow ì—”ì§„ ì‹œë„(ê°€ëŠ¥í•˜ë©´ ê°€ì¥ ë¹ ë¦„)
      Â· ì´ë•ŒëŠ” pandas ë²„ì „ì— ë”°ë¼ ì§€ì› ì¸ìê°€ ë‹¤ë¥´ë¯€ë¡œ ìµœì†Œ ì¸ìë§Œ ì „ë‹¬
      Â· dtype_backend='pyarrow'ë„ pandas 2.xì—ì„œë§Œ ê°€ëŠ¥ â†’ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
    - ì‹¤íŒ¨í•˜ë©´ pandas ê¸°ë³¸ ì—”ì§„ìœ¼ë¡œ í´ë°±
    """
    _ = _file_sig(path)  # íŒŒì¼ ë³€ê²½ ì‹œ ìºì‹œ ë¬´íš¨í™” í‚¤

    # 1) pyarrow ì—”ì§„ ìš°ì„  ì‹œë„ (ìˆì„ ë•Œë§Œ)
    if arrow:
        try:
            import pyarrow  # noqa: F401
            # (a) pandas 2.xì—ì„œë§Œ ë™ì‘í•˜ëŠ” ê²½ë¡œ
            try:
                return pd.read_csv(path, engine="pyarrow", encoding=encoding, dtype_backend="pyarrow")
            except TypeError:
                # (b) dtype_backend ë¯¸ì§€ì› â†’ ìµœì†Œ ì¸ìë§Œ
                return pd.read_csv(path, engine="pyarrow", encoding=encoding)
        except Exception:
            # pyarrow ë¯¸ì„¤ì¹˜/ë¯¸ì§€ì› â†’ ì•„ë˜ ê¸°ë³¸ ì—”ì§„ í´ë°±
            pass

    # 2) ê¸°ë³¸ ì—”ì§„ í´ë°±
    try:
        # pandas C ì—”ì§„ì—ì„œëŠ” memory_map/low_memoryê°€ ìœ íš¨
        return pd.read_csv(path, encoding=encoding, memory_map=memory_map, low_memory=low_memory)
    except TypeError:
        # í™˜ê²½ì— ë”°ë¼ ì¼ë¶€ ì¸ì ë¯¸ì§€ì› ì‹œ ìµœì†Œ ì¸ìë¡œ ì¬ì‹œë„
        return pd.read_csv(path, encoding=encoding)

df = read_csv_fast(DATA_PATH)
# st.write(f"ì½ì€ íŒŒì¼: {DATA_PATH.name}, shape={df.shape}")

# ----------------------------- 5) ì •ì œ -----------------------------
for col in ["ê°œë‹¹ê°€ê²©", "ì´ê³„ì•½ê¸ˆì•¡", "ê³„ì•½ë³´ì¦ê¸ˆ"]:
    if col in df.columns:
        # ë¶ˆí•„ìš” ë¬¸ì ì œê±° â†’ float
        s = (df[col].astype(str)
                 .str.replace(r"[^\d\-]", "", regex=True)
                 .replace("", np.nan))
        df[col] = pd.to_numeric(s, errors="coerce")

money_cols = ["ê°œë‹¹ ê°€ê²©", "ì´ê³„ì•½ê¸ˆì•¡", "ê³„ì•½ë³´ì¦ê¸ˆ"]
for c in money_cols:
    if c in df.columns:
        df[c] = df[c].apply(parse_money)

date_cols = ["ê³„ì•½ì¼", "ì´ì§€ê¸‰ì¼"]
for c in date_cols:
    if c in df.columns:
        df[c] = df[c].apply(to_datetime)

if {"ì´ì§€ê¸‰ì¼","ê³„ì•½ì¼"}.issubset(df.columns):
    df["ì§€ê¸‰ì§€ì—°ì¼ìˆ˜"] = (df["ì´ì§€ê¸‰ì¼"] - df["ê³„ì•½ì¼"]).dt.days
if {"ê³„ì•½ë³´ì¦ê¸ˆ","ì´ê³„ì•½ê¸ˆì•¡"}.issubset(df.columns):
    df["ë³´ì¦ê¸ˆìœ¨"] = df["ê³„ì•½ë³´ì¦ê¸ˆ"] / df["ì´ê³„ì•½ê¸ˆì•¡"]
if {"ì´ê³„ì•½ê¸ˆì•¡","ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)"}.issubset(df.columns):
    df["ì´ê³„ì•½ë‹¨ê°€"] = df["ì´ê³„ì•½ê¸ˆì•¡"] / df["ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)"].replace(0, np.nan)

df = df.sort_values("ê³„ì•½ì¼").reset_index(drop=True)
train_mask = df["ê³„ì•½ì¼"] <= CUTOFF_DATE
df_train = df[train_mask].copy()

# ----------------------------- 6) í”¼ì²˜ë§ -----------------------------
df_feat = df.copy()
df_feat = df_feat.loc[:, ~df_feat.columns.duplicated()]

# ë²”ì£¼ ë¹ˆë„(í›ˆë ¨ê¸°ê°„ ê¸°ì¤€) â†’ ì „ì²´ì— ë§¤í•‘
cat_cols = ["íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´", "ì œí’ˆêµ¬ë¶„", "ë°°í„°ë¦¬ì¢…ë¥˜", "ì§€ê¸‰í˜•íƒœ"]
for c in cat_cols:
    if c in df.columns:
        freq = df_train[c].value_counts()
        df_feat[f"{c}_freq"] = df_feat[c].map(freq).fillna(0)

# ì¼ë¶€ ë°ì´í„°ì…‹ì—ì„œ 'ì§€ê¸‰ í˜•íƒœ'ë¡œ í‘œê¸°ëœ ê²½ìš° í˜¸í™˜
if "ì§€ê¸‰í˜•íƒœ_freq" in df_feat.columns and "ì§€ê¸‰ í˜•íƒœ_freq" not in df_feat.columns:
    df_feat["ì§€ê¸‰ í˜•íƒœ_freq"] = df_feat["ì§€ê¸‰í˜•íƒœ_freq"]

df_feat["key_íŒë§¤êµ¬ë§¤"] = df_feat["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" + df_feat["êµ¬ë§¤ì—…ì²´"].astype(str)
df_feat["key_íŒë§¤ì œí’ˆë°°í„°ë¦¬"] = (
    df_feat["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" +
    df_feat["ì œí’ˆêµ¬ë¶„"].astype(str) + "âˆ¥" +
    df_feat["ë°°í„°ë¦¬ì¢…ë¥˜"].astype(str)
)

seen_íŒë§¤êµ¬ë§¤ = set(df_train["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" + df_train["êµ¬ë§¤ì—…ì²´"].astype(str))
seen_íŒë§¤ì œí’ˆë°°í„°ë¦¬ = set(
    df_train["íŒë§¤ì—…ì²´"].astype(str) + "âˆ¥" +
    df_train["ì œí’ˆêµ¬ë¶„"].astype(str) + "âˆ¥" +
    df_train["ë°°í„°ë¦¬ì¢…ë¥˜"].astype(str)
)
df_feat["ì‹ ê·œ_íŒë§¤êµ¬ë§¤"] = (~df_feat["key_íŒë§¤êµ¬ë§¤"].isin(seen_íŒë§¤êµ¬ë§¤)).astype(int)
df_feat["ì‹ ê·œ_íŒë§¤ì œí’ˆë°°í„°ë¦¬"] = (~df_feat["key_íŒë§¤ì œí’ˆë°°í„°ë¦¬"].isin(seen_íŒë§¤ì œí’ˆë°°í„°ë¦¬)).astype(int)

# íŒë§¤ì—…ì²´ ê¸°ì¤€ ê³¼ê±° ëˆ„ì  í†µê³„(ë²¡í„°í™” ë²„ì „)
for tcol, pfx in [
    ("ê°œë‹¹ê°€ê²©",        "íŒë§¤ì—…ì²´_unit_price"),
    ("ì´ê³„ì•½ê¸ˆì•¡",      "íŒë§¤ì—…ì²´_total_amount"),
    ("ë³´ì¦ê¸ˆìœ¨",        "íŒë§¤ì—…ì²´_deposit_rate"),
    ("ì§€ê¸‰ì§€ì—°ì¼ìˆ˜",     "íŒë§¤ì—…ì²´_delay_days"),
    ("ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)", "íŒë§¤ì—…ì²´_qty"),
]:
    if tcol in df_feat.columns:
        df_feat = add_expanding_stats(df_feat, ["íŒë§¤ì—…ì²´"], tcol, pfx)

# z-scoreë“¤
def _safe_series(name):
    return df_feat[name] if name in df_feat.columns else pd.Series(np.nan, index=df_feat.index)

df_feat["z_unit_price"]   = z_from(_safe_series("ê°œë‹¹ê°€ê²©"),      _safe_series("íŒë§¤ì—…ì²´_unit_price_mean"),   _safe_series("íŒë§¤ì—…ì²´_unit_price_std"))
df_feat["z_total_amount"] = z_from(_safe_series("ì´ê³„ì•½ê¸ˆì•¡"),    _safe_series("íŒë§¤ì—…ì²´_total_amount_mean"), _safe_series("íŒë§¤ì—…ì²´_total_amount_std"))
df_feat["z_deposit_rate"] = z_from(_safe_series("ë³´ì¦ê¸ˆìœ¨"),      _safe_series("íŒë§¤ì—…ì²´_deposit_rate_mean"), _safe_series("íŒë§¤ì—…ì²´_deposit_rate_std"))
df_feat["z_delay_days"]   = z_from(_safe_series("ì§€ê¸‰ì§€ì—°ì¼ìˆ˜"),  _safe_series("íŒë§¤ì—…ì²´_delay_days_mean"),   _safe_series("íŒë§¤ì—…ì²´_delay_days_std"))
df_feat["z_qty"]          = z_from(_safe_series("ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)"), _safe_series("íŒë§¤ì—…ì²´_qty_mean"),        _safe_series("íŒë§¤ì—…ì²´_qty_std"))

# íŒë§¤ì—…ì²´ë³„ ì´ì „ ê³ ìœ  ê±°ë˜ì²˜ ìˆ˜(ë²¡í„°í™”)
if {"íŒë§¤ì—…ì²´","êµ¬ë§¤ì—…ì²´","ê³„ì•½ì¼"}.issubset(df_feat.columns):
    df_feat["íŒë§¤ì—…ì²´_prior_partner_cnt"] = prior_partner_count(df_feat[["íŒë§¤ì—…ì²´","êµ¬ë§¤ì—…ì²´","ê³„ì•½ì¼"]].copy())
else:
    df_feat["íŒë§¤ì—…ì²´_prior_partner_cnt"] = 0.0

# í”¼ì²˜ ëª©ë¡(ì›ë³¸ ìœ ì§€, ëˆ„ë½ë¶„ì€ 0.0ìœ¼ë¡œ ì±„ì›€)
feature_cols = [
    "ê°œë‹¹ê°€ê²©","ì´ê³„ì•½ê¸ˆì•¡","ê³„ì•½ë³´ì¦ê¸ˆ","ì§€ê¸‰ì§€ì—°ì¼ìˆ˜","ë³´ì¦ê¸ˆìœ¨","ì´ê³„ì•½ë‹¨ê°€","ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)",
    "íŒë§¤ì—…ì²´_freq","êµ¬ë§¤ì—…ì²´_freq","ì œí’ˆêµ¬ë¶„_freq","ë°°í„°ë¦¬ì¢…ë¥˜_freq","ì§€ê¸‰ í˜•íƒœ_freq",
    "ì‹ ê·œ_íŒë§¤êµ¬ë§¤","ì‹ ê·œ_íŒë§¤ì œí’ˆë°°í„°ë¦¬",
    "z_unit_price","z_total_amount","z_deposit_rate","z_delay_days","z_qty",
    "íŒë§¤ì—…ì²´_unit_price_cnt","íŒë§¤ì—…ì²´_total_amount_cnt","íŒë§¤ì—…ì²´_deposit_rate_cnt","íŒë§¤ì—…ì²´_delay_days_cnt","íŒë§¤ì—…ì²´_qty_cnt",
    "íŒë§¤ì—…ì²´_prior_partner_cnt",
]
for c in feature_cols:
    if c not in df_feat.columns:
        df_feat[c] = 0.0

X_train = df_feat.loc[train_mask, feature_cols].astype(float).fillna(0.0).values
X_all   = df_feat[feature_cols].astype(float).fillna(0.0).values

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_all_s   = scaler.transform(X_all)

# ----------------------------- 7) Isolation Forest -----------------------------
if_model = IsolationForest(
    n_estimators=500,            # â† ëª¨ë¸ íŒŒë¼ë¯¸í„° ìœ ì§€
    contamination=CONTAMINATION,
    max_samples="auto",
    bootstrap=True,
    random_state=SEED,
    n_jobs=1
)
if_model.fit(X_train_s)
if_scores = -if_model.decision_function(X_all_s)
df_feat["if_score"] = if_scores
df_feat["if_score_norm"] = minmax(if_scores)

# ----------------------------- 8) (ì„ íƒ) LSTM ë³´ì¡° ìŠ¤ì½”ì–´ -----------------------------
df_feat["lstm_score_norm"] = np.nan
lstm_rows = []

if RUN_LSTM and TORCH_AVAILABLE:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    lstm_feature_cols = ["ê°œë‹¹ê°€ê²©","ë³´ì¦ê¸ˆìœ¨","ì§€ê¸‰ì§€ì—°ì¼ìˆ˜","ê³„ì•½ìˆ˜ëŸ‰(ë‹¨ìœ„ë‹¹)","ì´ê³„ì•½ê¸ˆì•¡"]

    counts = df_train["íŒë§¤ì—…ì²´"].value_counts()
    target_vendors = counts[counts >= MIN_TXN_FOR_LSTM].index.tolist()

    class _LSTMAE(nn.Module):
        def __init__(self, input_dim):
            super().__init__()
            self.model = LSTMAE(input_dim, hidden_size=LSTM_HIDDEN, num_layers=LSTM_LAYERS)
        def forward(self, x):
            return self.model(x)

    for vendor in target_vendors:
        g = df_feat[df_feat["íŒë§¤ì—…ì²´"] == vendor].sort_values("ê³„ì•½ì¼").copy()
        g_train = g[g["ê³„ì•½ì¼"] <= CUTOFF_DATE].copy()
        if g.shape[0] < SEQ_LEN + 1 or g_train.shape[0] < SEQ_LEN + 1:
            continue

        scaler_v = StandardScaler()
        g_train_feat = scaler_v.fit_transform(g_train[lstm_feature_cols].astype(float).fillna(0.0).values)
        g_all_feat   = scaler_v.transform(g[lstm_feature_cols].astype(float).fillna(0.0).values)

        train_seqs = make_sequences(g_train_feat, SEQ_LEN)
        all_seqs   = make_sequences(g_all_feat, SEQ_LEN)
        if train_seqs.shape[0] == 0 or all_seqs.shape[0] == 0:
            continue

        Xtr  = torch.tensor(train_seqs, dtype=torch.float32, device=device)
        Xall = torch.tensor(all_seqs,  dtype=torch.float32, device=device)

        model = _LSTMAE(len(lstm_feature_cols)).to(device)
        opt = torch.optim.Adam(model.parameters(), lr=LSTM_LR)
        loss_fn = nn.MSELoss()

        model.train()
        for _ in range(LSTM_EPOCHS):  # â† ì—í¬í¬ ìˆ˜ ìœ ì§€
            idx = torch.randperm(Xtr.size(0), device=device)
            for i in range(0, Xtr.size(0), LSTM_BATCH):
                sel = idx[i:i+LSTM_BATCH]
                batch = Xtr.index_select(0, sel)
                opt.zero_grad(set_to_none=True)
                recon = model(batch)
                loss = loss_fn(recon, batch)
                loss.backward()
                opt.step()

        model.eval()
        with torch.no_grad():
            tr_loss  = ((model(Xtr)  - Xtr)  ** 2).mean(dim=(1,2)).detach().cpu().numpy()
            all_loss = ((model(Xall) - Xall) ** 2).mean(dim=(1,2)).detach().cpu().numpy()

        idx_all = g.index.to_list()
        target_row_indices = idx_all[SEQ_LEN-1:]

        p50, p99 = np.percentile(tr_loss, 50), np.percentile(tr_loss, 99)
        denom = max(p99 - p50, 1e-6)
        norm_scores = np.clip((all_loss - p50) / denom, 0, 5) / 5.0

        tmp = pd.DataFrame({
            "íŒë§¤ì—…ì²´": vendor,
            "row_index": target_row_indices,
            "lstm_loss": all_loss,
            "lstm_score_norm": norm_scores
        })
        lstm_rows.append(tmp)

    if lstm_rows:
        lstm_df = pd.concat(lstm_rows, ignore_index=True)
        df_feat.loc[lstm_df["row_index"].values, "lstm_score_norm"] = lstm_df["lstm_score_norm"].values
else:
    lstm_df = pd.DataFrame(columns=["row_index","lstm_loss","lstm_score_norm","íŒë§¤ì—…ì²´"])

# ----------------------------- 9) ìµœì¢… ì ìˆ˜ ê²°í•© -----------------------------
if "final_score" in df_feat.columns:
    df_feat = df_feat.drop(columns=["final_score"])
# --------------------------------------------------------------------------
def combine_scores(a, b):
    if pd.isna(b):
        return a
    return max(0.6*a, 0.4*b)

df_feat["final_score"] = [
    combine_scores(a, b) for a, b in zip(df_feat["if_score_norm"], df_feat["lstm_score_norm"])
]
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10) Plotly ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
threshold = df_feat["final_score"].quantile(1 - CONTAMINATION)
df_feat["anomaly"] = df_feat["final_score"] >= threshold

fig_anom = px.scatter(
    df_feat,
    x="ê³„ì•½ì¼", y="final_score",
    color="anomaly",
    color_discrete_map={False: "lightblue", True: "red"},
    title=f"Anomalies (top {int(CONTAMINATION*100)}%) Highlighted",
    labels={"ê³„ì•½ì¼":"Contract Date", "final_score":"Final Score", "anomaly":"Anomaly"},
    hover_data={
        "ê³„ì•½ë²ˆí˜¸": True, "íŒë§¤ì—…ì²´": True, "êµ¬ë§¤ì—…ì²´": True,
        "ì œí’ˆêµ¬ë¶„": True, "ë°°í„°ë¦¬ì¢…ë¥˜": True, "final_score": ":.4f"
    }
)
fig_anom.update_traces(marker=dict(size=8))

# Streamlitì— Plotly ì°¨íŠ¸ë¡œ í‘œì‹œ
st.subheader("ğŸ” ì´ìƒì¹˜ ìŠ¤ì½”ì–´ ì‚°ì ë„")
st.plotly_chart(fig_anom, use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 11) ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
anom_df = df_feat[df_feat["final_score"] >= threshold]
anom_df = anom_df.loc[:, ~anom_df.columns.duplicated()]
cols = ["ê³„ì•½ë²ˆí˜¸", "ê³„ì•½ì¼", "íŒë§¤ì—…ì²´", "êµ¬ë§¤ì—…ì²´", "ì œí’ˆêµ¬ë¶„", "ë°°í„°ë¦¬ì¢…ë¥˜", "final_score"]
cols = [c for c in cols if c in anom_df.columns]
top_anom = anom_df[cols].sort_values("final_score", ascending=False)

st.subheader(f"ğŸš¨ Top {int(CONTAMINATION*100)}% ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸ ({len(top_anom)}ê±´)")
st.dataframe(top_anom, use_container_width=True)
